{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import skimage.io as io\n",
    "import pandas as pd\n",
    "from Dataset_classes.Flowers.Flowers_Dataset import Flowers102Dataset as Flowers_Dataset\n",
    "import torchvision\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "import skimage\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from clip import clip\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################################################# Files import  #################################################\n",
    "\n",
    "# Get current directory\n",
    "current_dir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "\n",
    "# Get the outer directory where Config.json file is located\n",
    "MLCOMP_dir = os.path.abspath(os.path.join(current_dir)) # os.pardir allows to go one directory back\n",
    "\n",
    "# Path to the config file\n",
    "# Config.json file contains the paths to the input dataset and output dataset directories\n",
    "config_path = os.path.join(MLCOMP_dir, 'Config.json')\n",
    "\n",
    "with open(config_path) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Get the statistics.json file path where the std and mean are stored\n",
    "statistics_path = config['statistics_flowers102']\n",
    "\n",
    "# Open the stats.json file and set the dictionary dataset_stats for normalization\n",
    "with open(statistics_path) as f:\n",
    "    dataset_stats = json.load(f)\n",
    "\n",
    "# Directory containing the images\n",
    "img_dir = config['output_dir_flowers102']\n",
    "\n",
    "# Paths to the labels file\n",
    "labels_file = config['output_labels_flowers102']\n",
    "\n",
    "# Paths to the setid file\n",
    "setid_file = config['output_setid_flowers102']\n",
    "\n",
    "flowers_names_file = config['flowers_name']\n",
    "\n",
    "with open(flowers_names_file) as f:\n",
    "    flowers_names = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################# Dataset preparation  #################################################\n",
    "dataset = Flowers_Dataset(csv_file = labels_file, root_dir= img_dir)\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [7000,1189])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/16\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 149,620,737\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", model.visual.input_resolution)\n",
    "print(\"Context length:\", model.context_length)\n",
    "print(\"Vocab size:\", model.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = set()\n",
    "\n",
    "for i in range(len(train_set)):\n",
    "    image, label = dataset.__getitem__(i)\n",
    "    image = torch.tensor(image)\n",
    "    images.append(image)\n",
    "    labels.add(flowers_names[str(int(label))])  # Assuming class names are stored as strings in the json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpine sea holly',\n",
       " 'anthurium',\n",
       " 'artichoke',\n",
       " 'azalea',\n",
       " 'ball moss',\n",
       " 'balloon flower',\n",
       " 'barbeton daisy',\n",
       " 'bearded iris',\n",
       " 'bee balm',\n",
       " 'bird of paradise',\n",
       " 'bishop of llandaff',\n",
       " 'black-eyed susan',\n",
       " 'buttercup',\n",
       " 'californian poppy',\n",
       " 'canna lily',\n",
       " 'canterbury bells',\n",
       " 'cape flower',\n",
       " 'carnation',\n",
       " 'cautleya spicata',\n",
       " 'clematis',\n",
       " \"colt's foot\",\n",
       " 'columbine',\n",
       " 'common dandelion',\n",
       " 'corn poppy',\n",
       " 'cyclamen',\n",
       " 'daffodil',\n",
       " 'desert-rose',\n",
       " 'english marigold',\n",
       " 'fire lily',\n",
       " 'frangipani',\n",
       " 'fritillary',\n",
       " 'garden phlox',\n",
       " 'gazania',\n",
       " 'geranium',\n",
       " 'giant white arum lily',\n",
       " 'globe-flower',\n",
       " 'grape hyacinth',\n",
       " 'great masterwort',\n",
       " 'hard-leaved pocket orchid',\n",
       " 'hibiscus',\n",
       " 'hippeastrum',\n",
       " 'king protea',\n",
       " 'lenten rose',\n",
       " 'lotus lotus',\n",
       " 'love in the mist',\n",
       " 'magnolia',\n",
       " 'marigold',\n",
       " 'mexican aster',\n",
       " 'monkshood',\n",
       " 'morning glory',\n",
       " 'orange dahlia',\n",
       " 'osteospermum',\n",
       " 'oxeye daisy',\n",
       " 'passion flower',\n",
       " 'pelargonium',\n",
       " 'peruvian lily',\n",
       " 'petunia',\n",
       " 'pincushion flower',\n",
       " 'pink primrose',\n",
       " 'pink-yellow dahlia',\n",
       " 'poinsettia',\n",
       " 'primula',\n",
       " 'prince of wales feathers',\n",
       " 'purple coneflower',\n",
       " 'red ginger',\n",
       " 'rose',\n",
       " 'ruby-lipped cattleya',\n",
       " 'silverbush',\n",
       " 'snapdragon',\n",
       " 'spear thistle',\n",
       " 'stemless gentian',\n",
       " 'sunflower',\n",
       " 'sweet pea',\n",
       " 'sweet william',\n",
       " 'sword lily',\n",
       " 'thorn apple',\n",
       " 'toad lily',\n",
       " 'tree mallow',\n",
       " 'tree poppy',\n",
       " 'wallflower',\n",
       " 'water lily',\n",
       " 'watercress',\n",
       " 'wild pansy',\n",
       " 'windflower',\n",
       " 'yellow iris'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7000, 3, 500, 500])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_tensor = torch.stack(images)\n",
    "images_tensor = images_tensor.permute(0, 3, 1, 2)\n",
    "images_tensor.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(images_tensor, texts, device: str):\n",
    "\n",
    "  # preprocess the texts to transform from text to tensors\n",
    "  images_tensor.to(device)\n",
    "  text_tokens = clip.tokenize([\"A photo of \" + desc for desc in texts]).to(device)\n",
    "\n",
    "  # encode the inputs\n",
    "  with torch.no_grad():\n",
    "    images_z = model.encode_image(images_tensor).float()\n",
    "    texts_z = model.encode_text(text_tokens).float()\n",
    "\n",
    "  return images_z, texts_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def cosine_similarity(images_z: torch.Tensor, texts_z: torch.Tensor):\n",
    "  # normalise the image and the text\n",
    "  images_z /= images_z.norm(dim=-1, keepdim=True)\n",
    "  texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
    "\n",
    "  # compute the dot product between the image and the text\n",
    "  similarity = (texts_z @ images_z.T)\n",
    "\n",
    "  return similarity.cpu()\n",
    "\n",
    "images_z, texts_z = encode_data(images_tensor, labels, device)\n",
    "similarity = cosine_similarity(images_z, texts_z)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(images_fp: list[str], texts: list[str], device: str):\n",
    "  # preprocess the images to transform from filenames to images to tensors\n",
    "  images = [preprocess(Image.open(image)) for image in images_fp]\n",
    "\n",
    "  # preprocess the texts to transform from text to tensors\n",
    "  images = torch.tensor(np.stack(images)).to(device)\n",
    "  text_tokens = clip.tokenize([\"A photo of \" + desc for desc in texts]).to(device)\n",
    "\n",
    "  # encode the inputs\n",
    "  with torch.no_grad():\n",
    "    images_z = model.encode_image(images).float()\n",
    "    texts_z = model.encode_text(text_tokens).float()\n",
    "\n",
    "  return images_z, texts_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(images_fp: list[str], texts: list[str], device: str):\n",
    "  # preprocess the images to transform from filenames to images to tensors\n",
    "  images = [preprocess(Image.open(image)) for image in images_fp]\n",
    "\n",
    "  # preprocess the texts to transform from text to tensors\n",
    "  images = torch.tensor(np.stack(images)).to(device)\n",
    "  text_tokens = clip.tokenize([\"A photo of \" + desc for desc in texts]).to(device)\n",
    "\n",
    "  # encode the inputs\n",
    "  with torch.no_grad():\n",
    "    images_z = model.encode_image(images).float()\n",
    "    texts_z = model.encode_text(text_tokens).float()\n",
    "\n",
    "  return images_z, texts_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = set()\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    image, label = dataset.__getitem__(i)\n",
    "    image = torch.tensor(image)\n",
    "    images.append(image)\n",
    "    labels.add(flowers_names[str(int(label))])  # Assuming class names are stored as strings in the json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_tensor = torch.stack(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8189, 500, 500, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8189, 3, 500, 500])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(images: list[torch.Tensor], texts: list[str], device: str, model):\n",
    "    text_tokens = clip.tokenize([\"A photo of \" + desc for desc in texts]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        images_z = model.encode_image(images).float()\n",
    "        texts_z = model.encode_text(text_tokens).float()\n",
    "\n",
    "    return images_z, texts_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def cosine_similarity(images_z: torch.Tensor, texts_z: torch.Tensor):\n",
    "    images_z /= images_z.norm(dim=-1, keepdim=True)\n",
    "    texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    similarity = (texts_z @ images_z.T)\n",
    "\n",
    "    return similarity.cpu()\n",
    "\n",
    "images_z, texts_z = encode_data(images_tensor, labels, device, model)\n",
    "similarity = cosine_similarity(images_z, texts_z)\n",
    "print(similarity)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
